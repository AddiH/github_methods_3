---
title: "practical_exercise_1, Methods 3, 2021, autumn semester"
author: 'Luke Ring'
date: "2021-09-15"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load("tidyverse")
```

## Exercise 1
The general linear model: $Y = X \beta + \epsilon$:  
Do a linear regression, expressing fuel usage as a function of weight using the function __lm__  

1. extract $\hat{\beta}$, $Y$, $\hat{Y}$, $X$ and $\epsilon$ from __model__ (hint: have a look at the function __model.matrix__)  
```{r ex1_1}
data(mtcars)

m1 <- lm(formula = mpg ~ wt, data = mtcars)

summary(m1)

X <- model.matrix(m1)
beta_hat <- coef(m1)
Y <- mtcars$mpg
Y_hat <- predict(m1)
epsilon <- residuals(m1)
```
    i. create a plot that illustrates $Y$ and $\hat{Y}$ (if you are feeling ambitious, also include $\epsilon$ (hint: you can use the function __arrows__))
```{r ex1_1i}
ggplot(mtcars, aes(x = X[, "wt"])) +
  geom_point(aes(y = Y)) +
  geom_line(aes(y = Y_hat))
```

2. estimate $\beta$ for a quadratic model ($y = {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) using ordinary least squares _without_ using __lm__; $\hat{\beta} = {({X}^{T} X)}^{-1} {X}^{T} Y$ (hint: add a third column to $X$ from step 1)
```{r ex1_2}
X <- cbind(X, wt2 = mtcars$wt^2)
tX <- t(X)

beta_estimate <- (solve(tX %*% X) %*% tX %*% Y)
beta_estimate

Y_hat2 <-
  beta_estimate[3] * X[, "wt2"] +
  beta_estimate[2] * X[, "wt"] +
  beta_estimate[1]

episilon_quad <- Y - Y_hat2
```
3. compare your acquired $\hat{\beta}$ with the output of the corresponding quadratic model created using __lm__ (hint: use the function __I__, see details under help and the sub-section formula operators here: https://www.datacamp.com/community/tutorials/r-formula-tutorial)  
```{r ex1_3}
m_quad <- lm(mpg ~ wt + I(wt^2), data = mtcars)
anova(m1, m_quad)
```
    i. create a plot that illustrates $Y$ and $\hat{Y}$ (if you are feeling ambitious, also include $\epsilon$ (hint: you can use the function __arrows__))  
```{r ex1_3i}
ggplot(mtcars, aes(x = X[, "wt"])) +
  geom_point(aes(y = Y)) +
  geom_line(aes(y = Y_hat)) + 
  geom_smooth(aes(y = Y_hat2), method = lm, formula = y ~ x + I(x^2)) 

```

## Exercise 2
Compare the plotted quadratic fit to the linear fit  

1. which seems better?  

> The quadratic model seems to fit the values better along the entire sample.


2. calculate the sum of squared errors, (show the calculation based on $\epsilon$). Which fit has the lower sum?  
```{r ex2_2}
rss_m1 <- sum((Y - Y_hat)^2)

rss_quad <- sum((Y - Y_hat2)^2)

rss_m1
rss_quad
```
3. now make a cubic fit ($y = {\beta}_{3} x^3 + {\beta}_{2} x^2 + {\beta}_{1} x + {\beta}_{0}$) and compare it to the quadratic fit  
```{r ex2_3}
X <- cbind(X, wt3 = mtcars$wt^3)
tX <- t(X)

beta_estimate_cube <- (solve(tX %*% X) %*% tX %*% Y)
beta_estimate_cube

Y_hat3 <- beta_estimate_cube[4] * X[, "wt3"] + beta_estimate_cube[3] * X[, "wt2"] + beta_estimate_cube[2] * X[, "wt"] + beta_estimate_cube[1]
episilon_cube <- Y - Y_hat3
```
    i. create a plot that illustrates $Y$ and $\hat{Y}$ for both the cubic and the quadratic fits (plot them in the same plot)  
```{r ex2_3i}
ggplot(mtcars, aes(x = X[, "wt"])) +
  geom_point(aes(y = Y)) +
  geom_smooth(aes(y = Y_hat2), method = lm, formula = y ~ x + I(x^2), color = "blue") +
  geom_smooth(aes(y = Y_hat3), method = lm, formula = y ~ x + I(x^2) + I(x^3), color = "red")
```
    ii. compare the sum of squared errors  
```{r ex2_3ii}
rss_cube <- sum((Y - Y_hat3)^2)

rss_quad

rss_cube
```
    iii. what's the estimated value of the "cubic" (${\beta}_3$) parameter? Comment on this!  
```{r ex2_3iii}

```

4. bonus question: which summary statistic is the fitted value (_Intercept_ or ${\beta}_0$ in $y = {\beta}_0$) below identical to?
```{r ex2_4, echo=FALSE}
lm(mpg ~ 1, data = mtcars)
```
## Exercise 3
Doing a logistic regression - estimating the probability that a car has automatic transmission (0) or manual transmission (1) based on its weight
```{r ex3}
data(mtcars)
logistic.model <- glm(formula = am ~ wt, data = mtcars, family = "binomial")
summary(logistic.model)
```

Probabilities live on the range $(0, 1)$ - using the so-called logit function as a "link-function" we can map these onto the range $(-\infty, \infty)$, i.e. the real numbers.  
  
What we model in this case is: $Pr(y = 1) = logit^{-1}(X \beta)$, i.e. the probability of a car having manual transmission, given its weight. $X \beta$ is called the linear predictor; compare with $Y = X \beta + \epsilon$ 
It is helpful to define the logit function and its inverse function for the following:  

```{r}
logit <- function(x) log(x / (1 - x))
inv.logit <- function(x) exp(x) / (1 + exp(x))
```

1. plot the fitted values for __logistic.model__:  
    i. what is the relation between the __linear.predictors__ and the __fitted_values__ of the __logistic.model__ object?

```{r ex3_1}
plot(logistic.model)
fv <- fitted.values(logistic.model)
lp <- coef(logistic.model)
lp

ggplot(mtcars, aes(x = wt))

fv
```
2. plot the logistic function, you've estimated based on your $\hat{\beta}$, (not just the fitted values). Use an _xlim_ of (0, 7)
    i. what's the interpretation of the estimated $\hat{\beta}_0$ (the _Intercept_)
    ii. calculate the estimated probability that the Pontiac Firebird has automatic transmission, given its weight
    iii. bonus question - plot the logistic function and highlight all the cars where we guessed wrongly, if we used the following "quantizer" function:
    
\begin{equation}
  transmission_{guess}=
  \begin{cases}
    1 (manual), & \text{if}\ PR(y = 1) â‰¥ 0.5 \\
    0 (automatic), & \text{otherwise}
  \end{cases}
\end{equation}    
    
3. plot quadratic fit alongside linear fit  
    i. judging visually, does adding a quadratic term make a difference?
    ii. check the details in the help of the AIC function - which of the models provide the better fit according to the AIC values and the residual deviance respectively?
    iii. in your own words, why might it be good to penalise a model like the quadratic model, we just fitted.
    
# Next time
We are going to looking at extending our models with so called random effects. We need to install the package "lme4" for this. Run the code below or install it from your package manager (Linux)  
```{r, eval=FALSE}
install.packages("lme4")
```
We can fit a model like this:

```{r}
library(lme4)
mixed.model <- lmer(mpg ~ wt + (1 | cyl), data = mtcars)
```

They result in plots like these:
```{r}
par(font.lab = 2, font.axis = 2, cex = 1.2)
plot(mtcars$wt, fitted.values(mixed.model),
  main = "Linear regression with group intercepts (n cylinders)",
  xlab = "Weight (lb/1000)", ylab = "Miles/(US) gallon",
  pch = 3
)
```

and this
```{r}
mixed.model <- lmer(mpg ~ wt + (wt | cyl), data = mtcars)
plot(mtcars$wt, fitted.values(mixed.model),
  main = "Linear regression with group intercepts and group slopes (n cylinders)",
  xlab = "Weight (lb/1000)", ylab = "Miles/(US) gallon",
  pch = 3
)
``` 

but also new warnings like:  

Warning:
In checkConv(attr(opt, "derivs"), opt\$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0121962 (tol = 0.002, component 1)
